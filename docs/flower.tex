%\documentclass[conference]{IEEEtran}
\documentclass[sigconf,edbt]{acmart-edbt-workshops}


\usepackage{ifpdf}


\usepackage{verbatim,latexsym} % ,psfig} % latexsym has \Join
\usepackage{array,graphicx} %,multicol} %,comath}
\usepackage{longtable}% for long tables
\usepackage{latexsym}
\usepackage{verbatim,amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{color}
\definecolor{mygray}{rgb}{0.95,0.95,0.95}
\usepackage{listings}
\lstset{
backgroundcolor=\color{mygray},
basicstyle=\footnotesize\ttfamily,
%frame=L,
}
\usepackage{microtype}
\newcommand{\+}{\discretionary{\mbox{${\bm\cdot}\mkern-1mu$}}{}{}}
\def \hfillx {\hspace*{-\textwidth} \hfill}
\setlength{\belowcaptionskip}{-1pt}
\setlength{\textfloatsep}{5pt}
\captionsetup{belowskip=0pt}
\hyphenation{op-tical net-works semi-conduc-tor}

\pagestyle{plain}

\begin{document}

\title{FLOWER: Data Flow in ER Diagrams}

\begin{comment}
 \author{
\IEEEauthorblockN{Carlos Ordonez\IEEEauthorrefmark{1}, 
Ladjel Bellatreche\IEEEauthorrefmark{2} }

\IEEEauthorblockA{\IEEEauthorrefmark{1}University of Houston, USA.
\IEEEauthorrefmark{2}LIAS/ISAE-ENSMA, Poitiers, France.
} 
}
\end{comment}


% EDBT
\author{Carlos Ordonez}
\affiliation{
    University of Houston,
    USA
}
%       \affaddr{Dept. of Computer Science}\\
%       \affaddr{Houston, TX, USA}\\
       %\email{ordonez@cs.uh.edu}

\author{Ladjel Bellatreche}
\affiliation{
       LIAS/ISAE-ENSMA,
       France
}


% IEEE
\begin{comment}
\author{
\IEEEauthorblockN{Carlos Ordonez}           
\IEEEauthorblockA{University of Houston\\          
%\IEEEauthorblockA{University of Houston\textsuperscript{\textsection}\\          
USA}                                               
\and                                               
\IEEEauthorblockN{Ladjel Bellatreche}               
\IEEEauthorblockA{LIAS/ISAE-ENSMA\\          
Poitiers, France}}                                              
\end{comment}




%\begingroup\renewcommand\thefootnote{\textsection}
%\footnotetext{Department of Computer Science, University of Houston, Houston TX 77204, USA}
%\begingroup\renewcommand\thefootnote{\IEEEauthorrefmark{1}}
%\footnotetext{Contact Author: stahsin.cse@gmail.com }

\begin{abstract}
Data Science has brought up significant data processing
outside the database realm.
ER diagrams have a proven track record to represent data structure and relationships, 
beyond relational databases. 
ER diagrams provide abstraction, flexibility, and an intuitive visual representation of 
data structure richer than metadata.
ER diagram notation has evolved, incorporating UML diagram symbols.
In data science, data files need significant pre-processing 
before analysis them, resulting in a long sequence of complex data transformations.
Such transformations are computed in diverse languages
including Python, SQL, Java and R.
We argue flow diagrams remain the main tool to visualize the main modules
of a software or computer system or the main steps of an algorithm, 
showing rectangles with verbs (processing), 
connected by arrows (showing processing order or flow dependences).
We take a bold step: we propose a hybrid diagram, 
mixing data pre-processing flow and data structure (i.e. FLOW+ER=FLOWER),
based on modern UML notation.
Since we want to introduce a minimal change to ER diagram notation,
we extend relationships lines with an arrow, indicating processing flow
and we label entities coming from pre-processing with numbers and transformation operators.
Such diagram can be automatically created from programs processing diverse files.
Our novel FLOWER diagram can help data scientists navigate data pre-processing, 
providing an integrated view of pre-processing programs with a data-centric angle.
\end{abstract}

%\begin{IEEEkeywords}
%ER Diagran, Big Data, Metadata, Data Science, Data Transformation
%\end{IEEEkeywords}

\maketitle
%-------------------------------------------------------%
\section{Introduction}

A significant effort is required to pre-process data sets in big data analytics
because data sets come from diverse sources, they have different structure,
they come in different file formats and they are not integrated.
Hence data analysts need to collect, integrate, clean, merge, aggregate, and transform data files
before they can perform analysis. 
These data transformations create many intermediate files, tables in a disorganized manner. 
%On the other hand, Flow diagrams are a common tool to understand source code and programs.
%Rather 
%ones.
%We call it FLOWER because our diagram combines both ER diagram and Flow diagram.
ER diagrams have a proven track record to represent data structure and relationships, beyond databases. 
The ER diagram strengths are generality, flexibility, and intuitive visual representation. 
On the other hand, flow diagrams remain the main mechanism to visualize major components or main processing steps of a software system, but they are less useful to understand complex algorithms.
Several closely related works on ER diagram \cite{MCHM2014}, \cite{Guo2018}, Flow diagram \cite{BNT1986}, \cite{WADHR2005}, and data transformation \cite{SVBS2016}, \cite{PKJ2019} have been done by other researchers.
In this work, 
we present a simple and heterodox idea.
%
We propose a hybrid diagram, 
combining data flow and data structure,
which we call FLOWER.
The goal is to assist big data analysts in data pre-processing.% by combining the new intermediate entities with the existing 

% related work


%-------------------------------------------------------%
\section{Definitions}

\subsection{ER model}

Let $E=\{E_1,...,E_n\}$, be a set of $n$ entities, linked by relationships.
We follow modern UML notation, where entities are represented by rectangles
and relationships are shown by lines, with crowfeet on the ``many'' side.
Each entity has a list of attributes, where each attribute can be atomic or multivalued.
There exists an identifying set of attributes for each entity (i.e. a primary key, an object id).
Intuitively, entities correspond to objects in real life
and relationships to actions.
Therefore, we will use nouns as entity names and verbs to describe relationships.

\subsection{Database terms}

Our proposed diagram extensions are inspired by database principles, extending them to
data science and big data.
We use the following acronyms: PK (primary key), foreign key (FK).


\subsection{Data sets}

In our diagram there are input, intermediate and output entities.
In this work, we assume the input entities can be a bag (i.e. there may be repeated elements).
That is, they do not have a primary key.
However, when managing text files, it is common to parse and analyze files, line by line.
Therefore, a line number is a primary key.
In the case of images, the image file name is commonly a PK as well.

%-------------------------------------------------------%
\section{Proposed Hybrid Diagram }

We do not assume input files have keys in the traditional database sense,
but we assume data pre-processing programs do produce entity names, keys, attributes
and relationships.
that can be captured from source code (i.e. file names, variable names, function names).
That is, we propose reverse data design:
create a diagram (kind of automated model) after data has been pre-processed.

\subsection{Extending ER diagram notation} 

We allow relationships lines to have an arrow 
indicating data flow direction.
This direction can also be interpreted as input and output, going from input entities
to output entities.
The arrow is shown only for data pre-processing entities (akin to denormalized or derived tables
in a relational database).
That is, in the particular case of ``raw'' data sets that can be represented 
as normalized tables there is no arrow between entities.

The arrow represents:
\begin{enumerate}
\item A processing dependence between two entities.  
\item Data flow direction. This direction indicates one entity is used as 
input.
\end{enumerate}

This is a minor, yet powerful, change that enables navigating all data elements in the data lake,
as well as having a data-oriented flow of big data processing.
We emphasize that the entities remain linked by keys.
In particular, a traditional ER diagram for a relational database still has PKs and FKs.

\subsection{Entites beyond Databases}

Our entity concept is broad:
entities can represent a file, matrix, relational table, or dataframe.
That is, we go beyond relational databases.
Our proposed diagram can be considered a metadata representation
for big data from a variety angle (i.e. diverse information, in different formats).


Entities are classified as:

\begin{itemize}
\item source (raw) entities, representing raw data,
loaded into the Data Lake

\item Transformation (data preparation, data pre-processing) entities being the output of some 
system (Hadoop), tool (statistics or machine learning) or
some programming language used in data science (Python, R, SQL).
\end{itemize}

We focus on representing data transformations for big data analytics,
including machine learning, graphs, and even text files (documents).
However, our diagram does not include (yet) the "analytic output"
such as the parameters of the ML model, IR metrics like precision/recall, graph metrics.
%We may need to pre-process the data set to represent the entities.
We propose these three major categories of data transformations: 

\begin{enumerate}
\item Merge, which splices multiple entities by some attribute, 
which is a generalized relational join operator. 

\item GroupBy, which partitions and aggregates records based on some key. 
We must emphasize Data Science languages (Python, R) 
provide operators or functions highly similar to the SQL GROUP BY  clause.
Notice also GROUP BY produces data sets with a PK.
In fact, this is the most common mechanism to eliminate and count duplicates in SQL.

\item Derived expressions, which represent derived attributes 
coming from a combination of functions and value-level operators 
(e.g. equations, string manipulation, arithmetic expression, nested function calls).
\end{enumerate}



%Our proposed solution has three modules.
%\vspace{-1mm}

First, our solution generates a preliminary diagram as follows. 
This diagram can be polished and customized by the analyst.
We show this process in Fig \ref{fig:expER}.


\begin{figure}[!hbt]
  \centering
\includegraphics[width=.95\linewidth,height=5.3cm]{erflow-exportER.jpg}
  \caption{High-level flow diagram.}
  \label{fig:expER}
\end{figure}




\begin{enumerate}
\item Importing ER diagrams available from existing transactional or data warehouse
databases.
We assume ER diagrams are available or can be easily constructed for a relational database DDLs
or exported from an ER diagram tool as CSV files.
%(2) Manual entity identification for files with non-relational data,
%extracting data set and attribute names from available metadata (file name, provenance, ontologies).

\item Automatic entity and attribute identification from metadata embedded in the file itself.
We assume CSV files are the default file format for spreadsheet data,
logs and mathematical software. 
On the other hand, JSON is another common standard file format to exchange data
among many platforms.
JSON can be converted to a collection of CSV files and vice-versa.

\item For plain text files like documents, source code we assume they contain
strings for words, numbers, symbols and so on. In this case, we assume an IR library or tool
will pre-process the file and convert it tables, matrices or data frames.
Then we propose to extract entity and attribute names from the final table, matrix or data frame.

\item Automatic data set and attribute name identification for data sets built by Python, R, or SQL code, generalizing a previous approach with SQL queries \cite{LOBK2019}, kind of ``reverse engineering". 
\end{enumerate}



% Automating step (3) is our major contribution.
Following database principles,
the diagram data is stored in two JSON files, 
where the first file contains the relationships and the second one contains the entities and their attributes. 
%Both Python and R has libraries to read from JSON files.
%The output from this module goes as input to the next module where we extract the detailed structure.
In the data transformation module, we define the transformation type and create new transformed entities. 
Data scientists 
may perform several transformations discussed above in the source code that generates a temporary entity. 
%Also, mathematical transformations like computing logarithm or others are supported in these languages.
%Also, the user may perform "Group by" on a entity by one or several attributes.
In the case of ``Merge", the entity structure may change but the attribute values remain the same, and the "Group by" may use one or more grouping attributes along with or without aggregations (sum, count, avg).
In general, aggregations return numbers, 
but using only ``Group by" will return the attribute values as their types.
Mathematical transformations will mostly return derived attributes.
Now, the new transformed entities are linked with the original entities using an arrow. 
%This helps the user to follow the chain of transformations, have a data orient
After each valid transformation step, we can store the newly generated FLOWER diagram in JSON files.
This FLOWER diagram can help the analysts to have data-oriented view of the program, navigate source code, reuse functions, and avoid creating redundant data sets.

\begin{figure*}[t]
  \centering
\includegraphics[width=.9\linewidth,height=6.3cm]{erflow-erflow.jpg}
  \caption{FLOWER diagram for a Store Data Lake (white entities are 
original raw entities, black entities represent data transformations).}
  \label{fig:erflow}
\end{figure*}

%Now, we present a case study based on a super-store data set.
We show an example in Figure \ref{fig:erflow} where we show our final FLOWER diagram. 
We consider an example of a store for which we show the FLOWER diagram. 
In our example the target analytic is a predictive model of product sales
considering history sales data, customer information and buyers' opinions.
The goal is to produce a data set, which can be used as input for 
a predictive model like regression, decision trees, SVMs or deep neural networks.
Each entity from the original data has an identifying attribute (primary key) and other attributes.
From these entities, analysts can generate new entities by doing data transformations as mentioned above.
Popular analytic languages like Python and R, both support data transformations (ex: ``Merge", ``Group by") in pandas and dpylr libraries respectively.
Each of the transformations generates a new entity which is named from the input entities and the transformation type is shown inside the parenthesis as ``(TYPE)".
%For example, new entity created by ``merge" from ``department" and ``product" entity is named as ``Merge\ E1".
The source entities are colored white and the transformed entities are colored grey for better understanding.
We can see the flow of the transformed entities as they are linked with an arrow from the source entities.

\begin{comment}
 experimental evaluation?
 measure abstraction, ease of use, size
 speed does nort make sense
 correctness does not make sense as there is no reference
\end{comment} 


\section{Related Work}

This is not the first work that tackles visual representation of big data pre-processing.
Data transformations represent valuable knowledge to reuse data sets
and
modern analysts have tons of source code which is impossible to understand.
Such transformations have already been proposed for relational databases 
by extending the ER models with transformation entities \cite{LOBK2019,OMMC2013}.

In an alternative line of work, 
\cite{PKJ2019} solved data transformation understanding with minimal human interaction.
A layout algorithm for automatic drawing of the data flow diagram was presented in \cite{BNT1986}.
This layout algorithms receives an abstract graph specifying connectivity relations between the elements as input, and produces a corresponding diagram as output.
%Before that, 
Wohed et. al in \cite{WADHR2005} analyzed the strengths and weaknesses of control flow specification in Activity Diagrams of UML.
The authors of \cite{MCHM2014} presented a method for entity resolution that infers relationships between observed entities and uses those relationships to aid mapping identities to underlying entities.


\section{Conclusions}

We believe data science and big data need
new tools to capture data structure and interrelationships,
following and extending proven database design techniques: ER diagrams
and relational databases.
We presented a preliminary idea, suitable for a workshop.
Evidently, there is a lot of work to do:
automating entity identification from Python/SQL scripts,
identifying entity relationships via input/output files
in source code,
producing a list of entities in UML notation
and applying our idea on actual data science projects to get
users feedback.


\bibliographystyle{IEEEtran}
\bibliography{db}


\end{document}



