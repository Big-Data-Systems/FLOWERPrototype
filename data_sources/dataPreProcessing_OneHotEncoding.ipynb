{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pylab as plt \n",
    "tf.config.run_functions_eagerly(run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/kaggle/input/imdbcomplete/aclImdb/train\"\n",
    "val_dir = \"/kaggle/input/imdbcomplete/aclImdb/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess input strings:\n",
    " ## Remove html tags\n",
    " ## remove punctuation, spaces\n",
    " ## Normalize words(Lemmatizer) Better=> Good\n",
    "def process_string(input):\n",
    "    output = input.lower()\n",
    "    output = re.sub(\"<[^>]+>\",'',output)\n",
    "    output = re.sub(\"[%s]\"%re.escape(string.punctuation),' ',output)\n",
    "    output = re.sub(\"  \",' ',output)\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    output = ' '.join([lemmatizer.lemmatize(w,pos=\"a\") for w in output.split()])\n",
    "    return output \n",
    "\n",
    "def pr_sent(input):\n",
    "    output = tf.strings.lower(input)\n",
    "    output =tf.strings.regex_replace(output,\"<[^>]+>\",\"\")\n",
    "    output =tf.strings.regex_replace(output,\"[%s]\"%re.escape(string.punctuation),' ')\n",
    "    output =tf.strings.regex_replace(output,\"  \",\" \")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "train_dataset_ = text_dataset_from_directory(train_dir, batch_size =8, shuffle=True, )\n",
    "val_dataset_ = text_dataset_from_directory(val_dir, batch_size =8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprcessing methods\n",
    "def pr(r,w):\n",
    "    return pr_sent(r),w\n",
    "train_dataset = train_dataset_.map(pr)\n",
    "val_dataset = val_dataset_.map(pr)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=32).cache().shuffle(buffer_size=25000)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=32).cache().shuffle(buffer_size=25000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization will transform words into onehot encoding style\n",
    "# with a vocabulary of 10 000 words and a sequence length of 100 w\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vec_layer = TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    output_mode ='int',\n",
    "    output_sequence_length =100\n",
    ")\n",
    "td = train_dataset.map(lambda x,y:x)\n",
    "vec_layer.adapt(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(review,label):\n",
    "    return tf.one_hot(vec_layer(review),depth=10000),label\n",
    "train_dataset = train_dataset.map(vectorizer)\n",
    "val_dataset = val_dataset.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the preprocessing phase\n",
    "for r,w in train_dataset.take(1):\n",
    "    print(r,w)\n",
    "print(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
